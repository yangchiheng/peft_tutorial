{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1001882c",
   "metadata": {},
   "source": [
    "# Fine-Tuning with Hugging Face PEFT\n",
    "**Goal**: This tutorial demonstrates how to fine-tune a pretrained transformer model using **Parameter-Efficient Fine-Tuning (PEFT)** with LoRA on a text classification task. The goal is to reduce computational cost while maintaining effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d02b32",
   "metadata": {},
   "source": [
    "## 1. Introduction to PEFT\n",
    "**Parameter-Efficient Fine-Tuning (PEFT)** is a suite of techniques that enables training only a small number of parameters.\n",
    "\n",
    "Common techniques include:\n",
    "- **LoRA**: Low-Rank Adaptation layers added to existing weights.\n",
    "- **Prefix Tuning**: Adds trainable vectors to transformer input sequences.\n",
    "- **Prompt Tuning**: Learns soft embeddings to prompt the model.\n",
    "\n",
    "### Why PEFT?\n",
    "- Reduces memory and compute requirements.\n",
    "- Supports multi-task fine-tuning without full model duplication.\n",
    "- Great for edge devices and low-budget environments.\n",
    "\n",
    "### Mathematical View: LoRA\n",
    "Let $W \\in \\mathbb{R}^{d \\times k}$ be a frozen pretrained weight matrix. We introduce trainable matrices $A \\in \\mathbb{R}^{d \\times r}$ and $B \\in \\mathbb{R}^{r \\times k}$. The update becomes:\n",
    "\n",
    "$W' = W + AB$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102d824c",
   "metadata": {},
   "source": [
    "## 2. Model Types and Selection\n",
    "- **Encoder-Only**: e.g., BERT, RoBERTa, DistilBERT – best for classification/token-level tasks\n",
    "- **Decoder-Only**: e.g., GPT-2, GPT-J, GPT-Neo – great for generative tasks\n",
    "- **Encoder-Decoder**: e.g., T5, BART, mT5 – good for seq2seq tasks like summarization or translation\n",
    "\n",
    "In this project, we use **encoder-only BERT** to perform **text classification** using LoRA from the PEFT library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07155d59",
   "metadata": {},
   "source": [
    "## 3. Environment Setup\n",
    "Install the required packages using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99c06d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (4.37.2)\n",
      "Requirement already satisfied: peft in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (0.10.0)\n",
      "Requirement already satisfied: datasets in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (2.18.0)\n",
      "Requirement already satisfied: accelerate in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (0.29.2)\n",
      "Requirement already satisfied: scikit-learn in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (1.5.2)\n",
      "Requirement already satisfied: torch in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (2.2.1+cu118)\n",
      "Requirement already satisfied: filelock in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: psutil in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from datasets) (2.2.1)\n",
      "Requirement already satisfied: xxhash in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from scikit-learn) (1.5.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (11.8.89)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (11.8.87)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.7.0.84 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (8.7.0.84)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (11.11.3.6)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (10.3.0.86)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (11.4.1.48)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (11.7.5.86)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.19.3 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from torch) (11.8.86)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/user/anaconda3/envs/henry3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers peft datasets accelerate scikit-learn torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58134823",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f147aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/henry3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "samples = [\n",
    "    {\"text\": \"I love this product!\", \"label\": 1},\n",
    "    {\"text\": \"Horrible experience.\", \"label\": 0},\n",
    "    {\"text\": \"Fantastic!\", \"label\": 1},\n",
    "    {\"text\": \"I want a refund.\", \"label\": 0},\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(samples)\n",
    "dataset = dataset.train_test_split(test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c638b1",
   "metadata": {},
   "source": [
    "## 5. Tokenization and Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4f48508",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 3/3 [00:00<00:00, 324.88 examples/s]\n",
      "Map: 100%|██████████| 1/1 [00:00<00:00, 261.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "def preprocess(example):\n",
    "    return tokenizer(example['text'], truncation=True, padding=\"max_length\", max_length=64)\n",
    "\n",
    "dataset = dataset.map(preprocess, batched=True)\n",
    "dataset = dataset.remove_columns([\"text\"])\n",
    "dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286c5e15",
   "metadata": {},
   "source": [
    "## 6. Apply LoRA with PEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf00506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 296,450 || all params: 109,780,228 || trainable%: 0.2700395193203643\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874d342",
   "metadata": {},
   "source": [
    "## 7. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd98ff38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/henry3/lib/python3.12/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]/home/user/anaconda3/envs/henry3/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "                                             \n",
      " 67%|██████▋   | 2/3 [00:03<00:01,  1.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6087948083877563, 'eval_runtime': 0.1352, 'eval_samples_per_second': 7.395, 'eval_steps_per_second': 7.395, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 3/3 [00:04<00:00,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.618904173374176, 'eval_runtime': 0.0535, 'eval_samples_per_second': 18.699, 'eval_steps_per_second': 18.699, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                             \n",
      "100%|██████████| 3/3 [00:04<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6238903403282166, 'eval_runtime': 0.0543, 'eval_samples_per_second': 18.416, 'eval_steps_per_second': 18.416, 'epoch': 3.0}\n",
      "{'train_runtime': 4.0898, 'train_samples_per_second': 2.201, 'train_steps_per_second': 0.734, 'train_loss': 0.7400258382161459, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=0.7400258382161459, metrics={'train_runtime': 4.0898, 'train_samples_per_second': 2.201, 'train_steps_per_second': 0.734, 'train_loss': 0.7400258382161459, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Define training arguments with W&B logging disabled properly\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19331bad",
   "metadata": {},
   "source": [
    "## 8. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b11e6f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/henry3/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:105: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n",
      "The model 'PeftModelForSequenceClassification' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: Fantastic!\n",
      "Predicted label: LABEL_1 with score 0.5084\n",
      "\n",
      "Input: I want a refund.\n",
      "Predicted label: LABEL_1 with score 0.5367\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create an inference pipeline using the trained model and tokenizer\n",
    "clf_pipeline = pipeline(\"text-classification\", model=model, tokenizer=tokenizer, return_all_scores=False)\n",
    "\n",
    "# Raw test examples (re-adding original text manually since it was removed from dataset)\n",
    "test_texts = [\"Fantastic!\", \"I want a refund.\"]\n",
    "\n",
    "# Run inference\n",
    "for text in test_texts:\n",
    "    prediction = clf_pipeline(text)[0]\n",
    "    print(f\"Input: {text}\")\n",
    "    print(f\"Predicted label: {prediction['label']} with score {prediction['score']:.4f}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95200d",
   "metadata": {},
   "source": [
    "## 9. Summary and Use Cases\n",
    "LoRA fine-tuning allows fast adaptation of large models in resource-constrained environments.\n",
    "\n",
    "**Use cases:**\n",
    "- Fine-tuning foundation models with small datasets\n",
    "- Efficient multi-task training\n",
    "- On-device NLP applications"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henry3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
