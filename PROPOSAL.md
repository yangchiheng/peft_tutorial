# Project Proposal

## Option Selected:
Option 1: Accessible Technical Tutorial for an NLP Tool

## Tool:
Hugging Face PEFT (Parameter-Efficient Fine-Tuning)

## Summary of Tool:
PEFT (Parameter-Efficient Fine-Tuning) is a Hugging Face library that enables efficient adaptation of large language models to specific tasks by training only a small subset of parameters. It supports common methods like LoRA (Low-Rank Adaptation), Prefix Tuning, and Prompt Tuning.

## Tutorial Plan:
This tutorial will include:
- An introduction to parameter-efficient fine-tuning and the motivation behind it.
- A brief overview of the PEFT library and techniques like LoRA.
- Instructions for installing the PEFT library and setting up the environment.
- A step-by-step guide to loading a pretrained model (e.g., `t5-small`) and preparing a small text classification dataset.
- Demonstrating how to apply a LoRA adapter to the model using PEFT.
- A discussion of the benefits and potential use cases of PEFT-based fine-tuning.

